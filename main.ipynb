{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9521835e-b648-4185-851e-61325f119c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "#The main over-arching strucutre of the system, where the forest contains a number of trees set as n_estimator\n",
    "#Also contains the max depth and the minimum samples per split\n",
    "class RandomForest:\n",
    "    #Initialize the constants of the random forest\n",
    "    def __init__(self, n_estimator=100, max_depth=None, min_samples_split=2):\n",
    "        self.n_estimator = n_estimator\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.trees = []\n",
    "    #Iterates through each tree number, creates a decision tree and populates with randomly sampled points - with replacement\n",
    "    #Then train/fit the decision tree on that sampled data, and append to the forest\n",
    "    def fit(self, X, y):\n",
    "        for _ in range(self.n_estimator):\n",
    "            tree = DecisionTree(max_depth=self.max_depth, min_samples_split=self.min_samples_split)\n",
    "            indices = np.random.choice(len(X), len(X), replace=True)\n",
    "            tree.fit(X[indices], y[indices])\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    #Creates a np-array that matches dim of the trees, iterates through the trees and predicts based on trees average\n",
    "    def predict(self, X):\n",
    "        predictions = np.zeros((len(X), len(self.trees)))\n",
    "        for i, tree in enumerate(self.trees):\n",
    "            predictions[:, i] = tree.predict(X)\n",
    "        return np.mean(predictions, axis=1)\n",
    "\n",
    "\n",
    "#Building block of each decision tree\n",
    "class Node:\n",
    "    #Initialize all constants of the Node\n",
    "    def __init__ (self, feature=None, threshold=None, left=None, right=None, value=None):\n",
    "        self.feature = feature #which feature to split on\n",
    "        self.threshold = threshold #cuttoff value to split\n",
    "        self.left = left #points to the left child node\n",
    "        self.right = right #points to the right child node\n",
    "        self.value = value #value of the current node\n",
    "\n",
    "\n",
    "#The building block for the forest, includes a structure for CART-style regression tree\n",
    "#splits data by minimizing variance \n",
    "class DecisionTree:\n",
    "    #Initializes with max depth and minimum number of samples per split \n",
    "    def __init__(self, max_depth=None, min_samples_split=2):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "    #Grow tree from the root node\n",
    "    def fit(self, X, y):\n",
    "        self.root = self._grow_tree(X,y)\n",
    "\n",
    "    #grow tree based on the shape of X\n",
    "    def _grow_tree(self, X, y, depth=0):\n",
    "        n_samples, n_features = X.shape\n",
    "        #If it surpasses max depth or the number of samples is below 2, create a new leaf node\n",
    "        if depth >= self.max_depth or len(y) < self.min_samples_split or len(set(y)) == 1:\n",
    "            return Node(value=np.mean(y))\n",
    "        #Finding the best split\n",
    "        #Set initial variance to infinity since any data will be less variance\n",
    "        best_variance = float('inf')\n",
    "        best_feature = best_threshold = None, None\n",
    "\n",
    "        #Go through each feature in the tree and assign the features to left or right node based on threshold\n",
    "        for feature in range(n_features):\n",
    "            thresholds = sorted(set(X[:, feature]))\n",
    "            for threshold in thresholds:\n",
    "                left_indices = X[:, feature] <= threshold\n",
    "                right_indices = X[:, feature] > threshold\n",
    "                #Skip any invalid splits and compute variance\n",
    "                if sum(left_indices) == 0 or sum(right_indices) == 0:\n",
    "                    continue\n",
    "                variance = self._calculate_variance(y[left_indices], y[right_indices])\n",
    "                #Keep the best splits/features if their variance is minimized\n",
    "                if variance < best_variance:\n",
    "                    best_variance = variance\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        #Recursively grow Tree\n",
    "        #If there is no split found, then select current node as a leaf\n",
    "        #If there is a split found, then continue to grow the tree as a root\n",
    "        if best_feature is None or best_threshold is None:\n",
    "            return Node(value=np.mean(y))\n",
    "\n",
    "        left_indices = X[:, best_feature] <= best_threshold\n",
    "        right_indices = X[:, best_feature] > best_threshold\n",
    "\n",
    "        left = self._grow_tree(X[left_indices], y[left_indices], depth + 1)\n",
    "        right = self._grow_tree(X[right_indices], y[right_indices], depth + 1)\n",
    "        return Node(feature=best_feature, threshold=best_threshold, left=left, right=right)\n",
    "    #Calculate variance\n",
    "    def _calculate_variance(self, left_value, right_value):\n",
    "        var_left = np.var(left_value)\n",
    "        var_right = np.var(right_value)\n",
    "        return (len(left_value) * var_left + len(right_value) * var_right) / (len(left_value) + len(right_value))\n",
    "\n",
    "    #Predict sample by traversing the tree\n",
    "    def predict(self, X):\n",
    "        return [self._traverse_tree(x, self.root) for x in X]\n",
    "    #Traverses tree from left to right, pre-order traversal\n",
    "    def _traverse_tree(self, x, node):\n",
    "        if node.value is not None:\n",
    "            return node.value\n",
    "        if x[node.feature] <= node.threshold:\n",
    "            return self._traverse_tree(x, node.left)\n",
    "        else:\n",
    "            return self._traverse_tree(x, node.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52be954d-d789-464b-bfd8-2ec5b394bf73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Load the dataset\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aaec2db5-10a7-4f3e-9ca3-95e7109dcf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the dataset into train and test\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c7451816-5841-48ae-a4e9-571a57a9044e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the random forest model on the training dataset\n",
    "#Since it uses a custom build random forest, takes much longer than sklearn built-in forest:\n",
    "#~5min for max_depth=5\n",
    "random_forest = RandomForest(n_estimator=100, max_depth=5, min_samples_split=2)\n",
    "random_forest.fit(X_train, y_train)\n",
    "\n",
    "#Predict the values of the test dataset using random forest\n",
    "predictions = random_forest.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ccd92346-5e02-4d6e-818e-7f5124705c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 2870.998742704578\n",
      "25.0 346.0 153.73654390934846\n"
     ]
    }
   ],
   "source": [
    "#Quanitfy the random forest's ability to predict test dataset\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean squared error: {mse}\")\n",
    "print(y_train.min(), y_train.max(), y_train.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d49edd9f-a13a-49b6-b546-87656a35654b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 2783.4670176611653\n"
     ]
    }
   ],
   "source": [
    "#Now, we can use sklearn random forest model as well\n",
    "#Regressor is used for continuous values and classifier is used for discrete \n",
    "#Completed in ~0.1sec with max_depth=5. The same value for mse, which means original structure was correct \n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "sk_rf = RandomForestRegressor(n_estimators=500, max_depth=3, random_state=42) \n",
    "sk_rf.fit(X_train, y_train) \n",
    "sk_predictions = sk_rf.predict(X_test) \n",
    "sk_mse = mean_squared_error(y_test, sk_predictions) \n",
    "print(f\"Mean squared error: {sk_mse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ad023d91-6e05-441c-8f8d-fdb894245dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 5, 'n_estimators': 500}\n",
      "Best CV mean squared error: 3380.2371224004464\n",
      "Test set MSE: 2863.96523655835\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Define grid of parameters to test\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7, 10, None]\n",
    "}\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42) \n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # sklearn maximizes score, so we use negative MSE\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and corresponding MSE\n",
    "best_params = grid_search.best_params_\n",
    "best_mse = -grid_search.best_score_\n",
    "\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best CV mean squared error:\", best_mse)\n",
    "\n",
    "# Train final model with best parameters\n",
    "best_rf = RandomForestRegressor(**best_params, random_state=42)\n",
    "best_rf.fit(X_train, y_train)\n",
    "y_pred = best_rf.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Test set MSE:\", test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e23e8b15-9ff7-4c64-960c-b1f5510f91cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['accuracy',\n",
       " 'adjusted_mutual_info_score',\n",
       " 'adjusted_rand_score',\n",
       " 'average_precision',\n",
       " 'balanced_accuracy',\n",
       " 'completeness_score',\n",
       " 'explained_variance',\n",
       " 'f1',\n",
       " 'f1_macro',\n",
       " 'f1_micro',\n",
       " 'f1_samples',\n",
       " 'f1_weighted',\n",
       " 'fowlkes_mallows_score',\n",
       " 'homogeneity_score',\n",
       " 'jaccard',\n",
       " 'jaccard_macro',\n",
       " 'jaccard_micro',\n",
       " 'jaccard_samples',\n",
       " 'jaccard_weighted',\n",
       " 'matthews_corrcoef',\n",
       " 'max_error',\n",
       " 'mutual_info_score',\n",
       " 'neg_brier_score',\n",
       " 'neg_log_loss',\n",
       " 'neg_mean_absolute_error',\n",
       " 'neg_mean_absolute_percentage_error',\n",
       " 'neg_mean_gamma_deviance',\n",
       " 'neg_mean_poisson_deviance',\n",
       " 'neg_mean_squared_error',\n",
       " 'neg_mean_squared_log_error',\n",
       " 'neg_median_absolute_error',\n",
       " 'neg_negative_likelihood_ratio',\n",
       " 'neg_root_mean_squared_error',\n",
       " 'normalized_mutual_info_score',\n",
       " 'positive_likelihood_ratio',\n",
       " 'precision',\n",
       " 'precision_macro',\n",
       " 'precision_micro',\n",
       " 'precision_samples',\n",
       " 'precision_weighted',\n",
       " 'r2',\n",
       " 'rand_score',\n",
       " 'recall',\n",
       " 'recall_macro',\n",
       " 'recall_micro',\n",
       " 'recall_samples',\n",
       " 'recall_weighted',\n",
       " 'roc_auc',\n",
       " 'roc_auc_ovo',\n",
       " 'roc_auc_ovo_weighted',\n",
       " 'roc_auc_ovr',\n",
       " 'roc_auc_ovr_weighted',\n",
       " 'top_k_accuracy',\n",
       " 'v_measure_score']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.metrics.get_scorer_names()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
